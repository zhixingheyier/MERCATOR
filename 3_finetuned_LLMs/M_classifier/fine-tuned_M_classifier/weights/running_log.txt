[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2058 >> loading file tokenizer.model

[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2058 >> loading file added_tokens.json

[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2058 >> loading file special_tokens_map.json

[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2058 >> loading file tokenizer_config.json

[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2058 >> loading file chat_template.jinja

[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2323 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|2025-05-27 22:06:08] configuration_utils.py:691 >> loading configuration file /media/guo/E/work/mywork_1/LLM_model/LLM-Research/Meta-Llama-3.1-8B/config.json

[INFO|2025-05-27 22:06:08] configuration_utils.py:765 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2058 >> loading file tokenizer.json

[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2058 >> loading file tokenizer.model

[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2058 >> loading file added_tokens.json

[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2058 >> loading file special_tokens_map.json

[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2058 >> loading file tokenizer_config.json

[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2058 >> loading file chat_template.jinja

[INFO|2025-05-27 22:06:08] tokenization_utils_base.py:2323 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

[INFO|2025-05-27 22:06:08] logging.py:143 >> Add pad token: <|end_of_text|>

[INFO|2025-05-27 22:06:08] logging.py:143 >> Loading dataset 5_27_train.json...

[INFO|2025-05-27 22:06:14] configuration_utils.py:691 >> loading configuration file /media/guo/E/work/mywork_1/LLM_model/LLM-Research/Meta-Llama-3.1-8B/config.json

[INFO|2025-05-27 22:06:14] configuration_utils.py:765 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 128256
}


[WARNING|2025-05-27 22:06:14] logging.py:148 >> FlashAttention-2 is not installed.

[INFO|2025-05-27 22:06:14] logging.py:143 >> Quantizing model to 4 bit with bitsandbytes.

[INFO|2025-05-27 22:06:14] logging.py:143 >> KV cache is disabled during training.

[INFO|2025-05-27 22:06:14] quantizer_bnb_4bit.py:279 >> The device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' 

[INFO|2025-05-27 22:06:14] modeling_utils.py:1121 >> loading weights file /media/guo/E/work/mywork_1/LLM_model/LLM-Research/Meta-Llama-3.1-8B/model.safetensors.index.json

[INFO|2025-05-27 22:06:14] modeling_utils.py:2167 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.

[INFO|2025-05-27 22:06:14] configuration_utils.py:1142 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "use_cache": false
}


[INFO|2025-05-27 22:06:29] modeling_utils.py:4930 >> All model checkpoint weights were used when initializing LlamaForCausalLM.


[INFO|2025-05-27 22:06:29] modeling_utils.py:4938 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /media/guo/E/work/mywork_1/LLM_model/LLM-Research/Meta-Llama-3.1-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

[INFO|2025-05-27 22:06:29] configuration_utils.py:1095 >> loading configuration file /media/guo/E/work/mywork_1/LLM_model/LLM-Research/Meta-Llama-3.1-8B/generation_config.json

[INFO|2025-05-27 22:06:29] configuration_utils.py:1142 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}


[INFO|2025-05-27 22:06:29] logging.py:143 >> Gradient checkpointing enabled.

[INFO|2025-05-27 22:06:29] logging.py:143 >> Using torch SDPA for faster training and inference.

[INFO|2025-05-27 22:06:29] logging.py:143 >> Upcasting trainable params to float32.

[INFO|2025-05-27 22:06:29] logging.py:143 >> Fine-tuning method: LoRA

[INFO|2025-05-27 22:06:29] logging.py:143 >> Found linear modules: up_proj,down_proj,gate_proj,o_proj,q_proj,v_proj,k_proj

[INFO|2025-05-27 22:06:30] logging.py:143 >> trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196

[INFO|2025-05-27 22:06:30] trainer.py:748 >> Using auto half precision backend

[WARNING|2025-05-27 22:06:30] trainer.py:783 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.

[WARNING|2025-05-27 22:06:30] trainer.py:783 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.

[WARNING|2025-05-27 22:06:30] trainer.py:783 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.

[WARNING|2025-05-27 22:06:30] trainer.py:783 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.

[INFO|2025-05-27 22:06:36] trainer.py:2414 >> ***** Running training *****

[INFO|2025-05-27 22:06:36] trainer.py:2415 >>   Num examples = 1,025

[INFO|2025-05-27 22:06:36] trainer.py:2416 >>   Num Epochs = 10

[INFO|2025-05-27 22:06:36] trainer.py:2417 >>   Instantaneous batch size per device = 1

[INFO|2025-05-27 22:06:36] trainer.py:2420 >>   Total train batch size (w. parallel, distributed & accumulation) = 32

[INFO|2025-05-27 22:06:36] trainer.py:2421 >>   Gradient Accumulation steps = 8

[INFO|2025-05-27 22:06:36] trainer.py:2422 >>   Total optimization steps = 320

[INFO|2025-05-27 22:06:36] trainer.py:2423 >>   Number of trainable parameters = 41,943,040

[INFO|2025-05-27 22:08:15] logging.py:143 >> {'loss': 7.5319, 'learning_rate': 4.0000e-05, 'epoch': 0.16, 'throughput': 443.71}

[INFO|2025-05-27 22:09:26] logging.py:143 >> {'loss': 0.8477, 'learning_rate': 4.9980e-05, 'epoch': 0.31, 'throughput': 515.10}

[INFO|2025-05-27 22:10:37] logging.py:143 >> {'loss': 0.3381, 'learning_rate': 4.9899e-05, 'epoch': 0.47, 'throughput': 549.04}

[INFO|2025-05-27 22:11:48] logging.py:143 >> {'loss': 0.1917, 'learning_rate': 4.9757e-05, 'epoch': 0.62, 'throughput': 569.52}

[INFO|2025-05-27 22:13:01] logging.py:143 >> {'loss': 0.4031, 'learning_rate': 4.9552e-05, 'epoch': 0.78, 'throughput': 602.03}

[INFO|2025-05-27 22:14:12] logging.py:143 >> {'loss': 0.1385, 'learning_rate': 4.9287e-05, 'epoch': 0.93, 'throughput': 609.48}

[INFO|2025-05-27 22:15:13] logging.py:143 >> {'loss': 0.1902, 'learning_rate': 4.8962e-05, 'epoch': 1.06, 'throughput': 613.71}

[INFO|2025-05-27 22:16:23] logging.py:143 >> {'loss': 0.1112, 'learning_rate': 4.8576e-05, 'epoch': 1.22, 'throughput': 616.49}

[INFO|2025-05-27 22:17:33] logging.py:143 >> {'loss': 0.0901, 'learning_rate': 4.8133e-05, 'epoch': 1.37, 'throughput': 613.62}

[INFO|2025-05-27 22:18:45] logging.py:143 >> {'loss': 0.0739, 'learning_rate': 4.7631e-05, 'epoch': 1.53, 'throughput': 614.15}

[INFO|2025-05-27 22:19:56] logging.py:143 >> {'loss': 0.0944, 'learning_rate': 4.7074e-05, 'epoch': 1.68, 'throughput': 622.26}

[INFO|2025-05-27 22:21:09] logging.py:143 >> {'loss': 0.1381, 'learning_rate': 4.6461e-05, 'epoch': 1.84, 'throughput': 625.55}

[INFO|2025-05-27 22:22:20] logging.py:143 >> {'loss': 0.0844, 'learning_rate': 4.5795e-05, 'epoch': 2.00, 'throughput': 626.71}

[INFO|2025-05-27 22:23:18] logging.py:143 >> {'loss': 0.0516, 'learning_rate': 4.5078e-05, 'epoch': 2.12, 'throughput': 627.15}

[INFO|2025-05-27 22:24:29] logging.py:143 >> {'loss': 0.0237, 'learning_rate': 4.4310e-05, 'epoch': 2.28, 'throughput': 626.28}

[INFO|2025-05-27 22:25:40] logging.py:143 >> {'loss': 0.0866, 'learning_rate': 4.3495e-05, 'epoch': 2.44, 'throughput': 627.99}

[INFO|2025-05-27 22:26:51] logging.py:143 >> {'loss': 0.0213, 'learning_rate': 4.2634e-05, 'epoch': 2.59, 'throughput': 626.22}

[INFO|2025-05-27 22:28:02] logging.py:143 >> {'loss': 0.0451, 'learning_rate': 4.1728e-05, 'epoch': 2.75, 'throughput': 630.03}

[INFO|2025-05-27 22:29:14] logging.py:143 >> {'loss': 0.0233, 'learning_rate': 4.0781e-05, 'epoch': 2.90, 'throughput': 628.22}

[INFO|2025-05-27 22:30:15] logging.py:143 >> {'loss': 0.0292, 'learning_rate': 3.9795e-05, 'epoch': 3.03, 'throughput': 632.52}

[INFO|2025-05-27 22:30:19] trainer.py:3984 >> Saving model checkpoint to saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-100

[INFO|2025-05-27 22:30:19] configuration_utils.py:691 >> loading configuration file /media/guo/E/work/mywork_1/LLM_model/LLM-Research/Meta-Llama-3.1-8B/config.json

[INFO|2025-05-27 22:30:19] configuration_utils.py:765 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-05-27 22:30:19] tokenization_utils_base.py:2510 >> tokenizer config file saved in saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-100/tokenizer_config.json

[INFO|2025-05-27 22:30:19] tokenization_utils_base.py:2519 >> Special tokens file saved in saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-100/special_tokens_map.json

[INFO|2025-05-27 22:31:34] logging.py:143 >> {'loss': 0.0307, 'learning_rate': 3.8772e-05, 'epoch': 3.19, 'throughput': 634.96}

[INFO|2025-05-27 22:32:44] logging.py:143 >> {'loss': 0.0251, 'learning_rate': 3.7715e-05, 'epoch': 3.34, 'throughput': 631.34}

[INFO|2025-05-27 22:33:55] logging.py:143 >> {'loss': 0.0696, 'learning_rate': 3.6627e-05, 'epoch': 3.50, 'throughput': 631.68}

[INFO|2025-05-27 22:35:08] logging.py:143 >> {'loss': 0.0227, 'learning_rate': 3.5509e-05, 'epoch': 3.65, 'throughput': 630.47}

[INFO|2025-05-27 22:36:18] logging.py:143 >> {'loss': 0.0253, 'learning_rate': 3.4365e-05, 'epoch': 3.81, 'throughput': 632.69}

[INFO|2025-05-27 22:37:29] logging.py:143 >> {'loss': 0.0201, 'learning_rate': 3.3198e-05, 'epoch': 3.96, 'throughput': 634.81}

[INFO|2025-05-27 22:38:28] logging.py:143 >> {'loss': 0.0335, 'learning_rate': 3.2011e-05, 'epoch': 4.09, 'throughput': 633.13}

[INFO|2025-05-27 22:39:40] logging.py:143 >> {'loss': 0.0108, 'learning_rate': 3.0806e-05, 'epoch': 4.25, 'throughput': 631.66}

[INFO|2025-05-27 22:40:52] logging.py:143 >> {'loss': 0.0030, 'learning_rate': 2.9587e-05, 'epoch': 4.40, 'throughput': 635.52}

[INFO|2025-05-27 22:42:04] logging.py:143 >> {'loss': 0.0040, 'learning_rate': 2.8356e-05, 'epoch': 4.56, 'throughput': 636.42}

[INFO|2025-05-27 22:43:15] logging.py:143 >> {'loss': 0.0003, 'learning_rate': 2.7117e-05, 'epoch': 4.72, 'throughput': 637.19}

[INFO|2025-05-27 22:44:26] logging.py:143 >> {'loss': 0.0019, 'learning_rate': 2.5872e-05, 'epoch': 4.87, 'throughput': 637.16}

[INFO|2025-05-27 22:45:25] logging.py:143 >> {'loss': 0.0012, 'learning_rate': 2.4626e-05, 'epoch': 5.00, 'throughput': 636.59}

[INFO|2025-05-27 22:46:36] logging.py:143 >> {'loss': 0.0001, 'learning_rate': 2.3380e-05, 'epoch': 5.16, 'throughput': 635.10}

[INFO|2025-05-27 22:47:47] logging.py:143 >> {'loss': 0.0072, 'learning_rate': 2.2139e-05, 'epoch': 5.31, 'throughput': 636.40}

[INFO|2025-05-27 22:49:00] logging.py:143 >> {'loss': 0.0001, 'learning_rate': 2.0905e-05, 'epoch': 5.47, 'throughput': 636.39}

[INFO|2025-05-27 22:50:11] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.9680e-05, 'epoch': 5.62, 'throughput': 637.40}

[INFO|2025-05-27 22:51:23] logging.py:143 >> {'loss': 0.0001, 'learning_rate': 1.8469e-05, 'epoch': 5.78, 'throughput': 638.62}

[INFO|2025-05-27 22:52:34] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.7275e-05, 'epoch': 5.93, 'throughput': 638.03}

[INFO|2025-05-27 22:53:35] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.6099e-05, 'epoch': 6.06, 'throughput': 638.06}

[INFO|2025-05-27 22:53:38] trainer.py:3984 >> Saving model checkpoint to saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-200

[INFO|2025-05-27 22:53:38] configuration_utils.py:691 >> loading configuration file /media/guo/E/work/mywork_1/LLM_model/LLM-Research/Meta-Llama-3.1-8B/config.json

[INFO|2025-05-27 22:53:38] configuration_utils.py:765 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-05-27 22:53:38] tokenization_utils_base.py:2510 >> tokenizer config file saved in saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-200/tokenizer_config.json

[INFO|2025-05-27 22:53:38] tokenization_utils_base.py:2519 >> Special tokens file saved in saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-200/special_tokens_map.json

[INFO|2025-05-27 22:54:51] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.4946e-05, 'epoch': 6.22, 'throughput': 636.02}

[INFO|2025-05-27 22:56:04] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.3817e-05, 'epoch': 6.37, 'throughput': 636.86}

[INFO|2025-05-27 22:57:14] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.2717e-05, 'epoch': 6.53, 'throughput': 636.82}

[INFO|2025-05-27 22:58:26] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.1646e-05, 'epoch': 6.68, 'throughput': 638.76}

[INFO|2025-05-27 22:59:37] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.0610e-05, 'epoch': 6.84, 'throughput': 638.19}

[INFO|2025-05-27 23:00:47] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 9.6085e-06, 'epoch': 7.00, 'throughput': 637.91}

[INFO|2025-05-27 23:01:45] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 8.6456e-06, 'epoch': 7.12, 'throughput': 638.50}

[INFO|2025-05-27 23:02:57] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 7.7234e-06, 'epoch': 7.28, 'throughput': 638.25}

[INFO|2025-05-27 23:04:08] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 6.8442e-06, 'epoch': 7.44, 'throughput': 638.49}

[INFO|2025-05-27 23:05:21] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 6.0101e-06, 'epoch': 7.59, 'throughput': 638.69}

[INFO|2025-05-27 23:06:32] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 5.2232e-06, 'epoch': 7.75, 'throughput': 638.42}

[INFO|2025-05-27 23:07:45] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 4.4855e-06, 'epoch': 7.90, 'throughput': 638.55}

[INFO|2025-05-27 23:08:44] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 3.7988e-06, 'epoch': 8.03, 'throughput': 638.59}

[INFO|2025-05-27 23:09:55] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 3.1648e-06, 'epoch': 8.19, 'throughput': 638.41}

[INFO|2025-05-27 23:11:07] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 2.5851e-06, 'epoch': 8.34, 'throughput': 638.58}

[INFO|2025-05-27 23:12:18] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 2.0611e-06, 'epoch': 8.50, 'throughput': 638.87}

[INFO|2025-05-27 23:13:29] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.5941e-06, 'epoch': 8.65, 'throughput': 639.40}

[INFO|2025-05-27 23:14:40] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.1854e-06, 'epoch': 8.81, 'throughput': 639.61}

[INFO|2025-05-27 23:15:53] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 8.3580e-07, 'epoch': 8.96, 'throughput': 639.08}

[INFO|2025-05-27 23:16:51] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 5.4631e-07, 'epoch': 9.09, 'throughput': 639.11}

[INFO|2025-05-27 23:16:54] trainer.py:3984 >> Saving model checkpoint to saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-300

[INFO|2025-05-27 23:16:54] configuration_utils.py:691 >> loading configuration file /media/guo/E/work/mywork_1/LLM_model/LLM-Research/Meta-Llama-3.1-8B/config.json

[INFO|2025-05-27 23:16:54] configuration_utils.py:765 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-05-27 23:16:54] tokenization_utils_base.py:2510 >> tokenizer config file saved in saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-300/tokenizer_config.json

[INFO|2025-05-27 23:16:54] tokenization_utils_base.py:2519 >> Special tokens file saved in saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-300/special_tokens_map.json

[INFO|2025-05-27 23:18:05] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 3.1762e-07, 'epoch': 9.25, 'throughput': 638.46}

[INFO|2025-05-27 23:19:18] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.5029e-07, 'epoch': 9.40, 'throughput': 639.20}

[INFO|2025-05-27 23:20:29] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 4.4747e-08, 'epoch': 9.56, 'throughput': 639.55}

[INFO|2025-05-27 23:21:42] logging.py:143 >> {'loss': 0.0000, 'learning_rate': 1.2433e-09, 'epoch': 9.72, 'throughput': 639.58}

[INFO|2025-05-27 23:21:45] trainer.py:3984 >> Saving model checkpoint to saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-320

[INFO|2025-05-27 23:21:45] configuration_utils.py:691 >> loading configuration file /media/guo/E/work/mywork_1/LLM_model/LLM-Research/Meta-Llama-3.1-8B/config.json

[INFO|2025-05-27 23:21:45] configuration_utils.py:765 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-05-27 23:21:45] tokenization_utils_base.py:2510 >> tokenizer config file saved in saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-320/tokenizer_config.json

[INFO|2025-05-27 23:21:45] tokenization_utils_base.py:2519 >> Special tokens file saved in saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/checkpoint-320/special_tokens_map.json

[INFO|2025-05-27 23:21:46] trainer.py:2681 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)



[INFO|2025-05-27 23:21:50] trainer.py:3984 >> Saving model checkpoint to saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps

[INFO|2025-05-27 23:21:50] configuration_utils.py:691 >> loading configuration file /media/guo/E/work/mywork_1/LLM_model/LLM-Research/Meta-Llama-3.1-8B/config.json

[INFO|2025-05-27 23:21:50] configuration_utils.py:765 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.3",
  "use_cache": true,
  "vocab_size": 128256
}


[INFO|2025-05-27 23:21:50] tokenization_utils_base.py:2510 >> tokenizer config file saved in saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/tokenizer_config.json

[INFO|2025-05-27 23:21:50] tokenization_utils_base.py:2519 >> Special tokens file saved in saves/Llama-3.1-8B/lora/train_2025-05-27-Llama-3.1-8B-10-epoch-320steps/special_tokens_map.json

[WARNING|2025-05-27 23:21:50] tuner.py:88 >> Failed to destroy process group: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:133, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 'out of memory'.

[WARNING|2025-05-27 23:21:50] logging.py:148 >> No metric eval_loss to plot.

[WARNING|2025-05-27 23:21:50] logging.py:148 >> No metric eval_accuracy to plot.

[INFO|2025-05-27 23:21:50] modelcard.py:450 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

